
from argparse import ArgumentParser
import torch
import spacy
import scispacy

nlp = spacy.load("en_core_sci_sm", disable=["tagger", "parser", "ner", "lemmatizer", "attribute_ruler"]) 
nlp.add_pipe("sentencizer")
nlp.max_length = 10_000_000

#To-DO
"""
1. compatibility with Hydra and ZenmL. Hydra loads the following params for`tokenize_and_align` func: 
tokenizer, label_all_tokens=True, device, max_length, 
2. Add func docstrings
3. C
"""

def tokenize_with_offsets(text: str, nlp:scispacy=nlp):
    doc = nlp(text)
    sentences = []
    for sent in doc.sents:
        sentence_tokens = []
        for token in sent:
            sentence_tokens.append({
                "text": token.text,
                "start": token.idx,
                "end": token.idx + len(token.text)
            })
        sentences.append(sentence_tokens)
    return sentences


def label_tokens_with_iob(sentences, entities):
    for sent in sentences:
        for token in sent:
            token["label"] = "O"  # default tag
            for ent in entities:
                if ent["start"] <= token["start"] < ent["end"]:
                    prefix = "B-" if token["start"] == ent["start"] else "I-"
                    token["label"] = f"{prefix}{ent['label']}"
                    break
    return sentences




def tokenize_and_align(examples, tokenizer, device, tag_col, label_all_tokens=True):
    """
    example is a huggingface dataset class
    Example has columns: [tokens, tags]
    """
    tokenized_inputs = tokenized_inputs = tokenizer(
        examples["tokens"],
        max_length=512,
        truncation=True,
        padding=True,
        is_split_into_words=True,
        return_tensors='pt'
    )
    labels = []

    for i,label in enumerate(examples[tag_col]):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        previous_word_idx = None
        label_ids = []

        for word_idx in word_ids: #iterate over word_ids list generated by tokenizer
            if word_idx is None:
                label_ids.append(-100) #ignore CLS and SEP tokens for loss func calculation
            elif word_idx != previous_word_idx:
                label_ids.append(label[word_idx]) #only label the first token of a given word(sub-parts are ignored)
            else:
            # For the other tokens in a word, we set the label to either the current label or -100, depending on
            # the label_all_tokens flag.
                label_ids.append(label[word_idx] if label_all_tokens else -100) 
            previous_word_idx = word_idx
        labels.append(label_ids)

        tokenized_inputs['labels'] = labels
        labels = torch.tensor(labels).to(dtype=torch.int64).to(device)  # Move labels to device
        return tokenized_inputs


