
from typing import List, Tuple, Dict, Union
from pathlib import Path
import random
import glob
from argparse import ArgumentParser

import torch
import spacy
import scispacy

from utils.file_writers import write_to_conll
from utils.file_parsers import load_brat
from utils.helper_functions import clean_text




nlp = spacy.load("en_core_sci_sm", disable=["tagger", "parser", "ner", "lemmatizer", "attribute_ruler"]) 
nlp.add_pipe("sentencizer")
nlp.max_length = 10_000_000

#To-DO
"""
1. compatibility with Hydra and ZenmL. Hydra loads the following params for`tokenize_and_align` func: 
tokenizer, label_all_tokens=True, device, max_length, 
2. Add func docstrings
3. C
"""

def tokenize_with_offsets(text: str, nlp:scispacy=nlp):
    doc = nlp(text)
    sentences = []
    for sent in doc.sents:
        sentence_tokens = []
        for token in sent:
            sentence_tokens.append({
                "text": token.text,
                "start": token.idx,
                "end": token.idx + len(token.text)
            })
        sentences.append(sentence_tokens)
    return sentences


def label_tokens_with_iob(sentences, entities):
    for sent in sentences:
        for token in sent:
            token["label"] = "O"  # default tag
            for ent in entities:
                if ent["start"] <= token["start"] < ent["end"]:
                    prefix = "B-" if token["start"] == ent["start"] else "I-"
                    token["label"] = f"{prefix}{ent['label']}"
                    break
    return sentences




def tokenize_and_align(*, examples, tokenizer, label_all_tokens=True, device, tag_col):
    """
    example is a huggingface dataset class
    Example has columns: [tokens, tags]
    """
    tokenized_inputs = tokenized_inputs = tokenizer(
        examples["tokens"],
        max_length=512,
        truncation=True,
        padding=True,
        is_split_into_words=True,
        return_tensors='pt'
    )
    labels = []

    for i,label in enumerate(examples[tag_col]):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        previous_word_idx = None
        label_ids = []

        for word_idx in word_ids: #iterate over word_ids list generated by tokenizer
            if word_idx is None:
                label_ids.append(-100) #ignore CLS and SEP tokens for loss func calculation
            elif word_idx != previous_word_idx:
                label_ids.append(label[word_idx]) #only label the first token of a given word(sub-parts are ignored)
            else:
            # For the other tokens in a word, we set the label to either the current label or -100, depending on
            # the label_all_tokens flag.
                label_ids.append(label[word_idx] if label_all_tokens else -100) 
            previous_word_idx = word_idx
        labels.append(label_ids)

        tokenized_inputs['labels'] = labels
        labels = torch.tensor(labels).to(dtype=torch.int64).to(device)  # Move labels to device
        return tokenized_inputs




def process_single_file_brat(file_id: str, input_dir: Path):
    """
    Process one BRAT-annotated document and return labeled tokenized sentences in IOB/BIO format.
    """
    txt_path = input_dir / f"{file_id}.txt"
    dataset = load_brat(txt_path)
    if not dataset:
        return None
    text, entities = dataset[0]["text"], dataset[0]["entities"]
    text = clean_text(text)
    tokenized_text = tokenize_with_offsets(text)
    labeled_IOB = label_tokens_with_iob(tokenized_text, entities)
    return labeled_IOB

def _process_split_brat(file_ids: List[str], input_dir: Path, output_dir: Path, split_name: str):
    """
    Process all files in a list, e.g train_splits: [1.txt, 2.txt,.......] and write them to disk.
    """
    for file_id in file_ids:
        labeled_sentences = process_single_file_brat(file_id, input_dir)
        if labeled_sentences:
            write_to_conll(labeled_sentences, output_dir, split_name)

def data_splitter_brat(path: str, train_ratio: float = 0.6, seed: int = 42) -> Dict[str, List[str]]:
    path = Path(path)
    all_files = list(path.glob("*.txt"))
    random.seed(seed)
    random.shuffle(all_files)
    base_filenames = [f.stem for f in all_files]

    total_files = len(base_filenames)
    train_size = int(total_files * train_ratio)
    val_test_size = (total_files - train_size) // 2

    return {
        "train": base_filenames[:train_size],
        "val": base_filenames[train_size:train_size + val_test_size],
        "test": base_filenames[train_size + val_test_size:]
    }

def process_dataset_brat(split_dict: Dict[str, List[str]], input_dir: Union[str, Path], output_dir: Union[str, Path]):
    """
    Process all data splits (train, test, val) using BRAT-to-CoNLL pipeline.
    Assuming input is a dict with split names as keys and files as values. 
    Example: {"train":[train_files......],
    "test":[test_files......]
    "val":[val_files......]
    }
    Only used to preprocess cell-finder dataset in our pipeline
    """
    input_dir = Path(input_dir)
    output_dir = Path(output_dir)

    for split_name, file_ids in split_dict.items():
        print(f"Processing {split_name} with {len(file_ids)} files...")
        _process_split_brat(file_ids, input_dir, output_dir, split_name)

