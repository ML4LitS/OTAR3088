  # Model & Tokenizer
  
name: null #synonymous to model_name_or_path. sticking to just "name" for uniformity across other configs in the pipeline
tokenizer_name: ${name}

# Training hyperparameters
learning_rate: ${lr}
per_device_train_batch_size: ${batch_size}
per_device_eval_batch_size: ${eval_batch_size}
num_train_epochs: ${num_epochs}
weight_decay: 0.01
warmup_steps: 500
lr_scheduler_type: linear

# Logging and evaluation
logging_steps: 100
evaluation_strategy: "epoch"
save_strategy: "epoch"
save_total_limit: 2
load_best_model_at_end: true
metric_for_best_model: "f1"

# Output paths
output_dir: null


# Optional params 
fp16: true #mixed precision for faster training on GPU
report_to: "wandb"  # or "wandb", "tensorboard", etc.



